res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json$errors$message
zapros
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json$errors$message
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
base <- "https://api.twitter.com/1.1/search/tweets.json?q="
word <- "#запарк"
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json$errors$message
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
base <- "https://api.twitter.com/1.1/search/tweets.json?q="
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
base <- "https://api.twitter.com/1.1/search/tweets.json?q="
word <- "#запарк"
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json$errors$message
apiKey <- "5ijhfi2ALa8JNCUp4Cd4Ntbxs"
apiSecret <- "HWSIAhl6sYWqkFxfCZFIaHttyRXt6iy9Ikhp1FVOPIhAKD0tM6"
access_token <- "114297502-BXqpFuIqdJ6mqRqfDLEEEXtZljoa6enN9m4sHC53"
access_token_secret <- "1F0t7xFfFy7ofkjZQyEIwbLKA0EZoemihDI1f6fI2kVFV"
setup_twitter_oauth(apiKey,apiSecret,access_token,access_token_secret)
tw = searchTwitter(word_enc,n=50,lang="ru")
twListToDF(tw)
res <- with_config(twitter_token, GET(zapros, handle = "twitter"))
res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json
twapp <- oauth_app("twappinfo", access_token, access_token_secret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
base <- "https://api.twitter.com/1.1/search/tweets.json?q="
word <- "#запарк"
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- with_config(twitter_token, GET(zapros))
res_json <- fromJSON(toJSON(content(res)))
res_json
res <- GET(zapros, config = twitter_token)
res_json <- fromJSON(toJSON(content(res)))
res_json$errors
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
base <- "https://api.twitter.com/1.1/search/tweets.json?q="
word <- "#запарк"
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- GET(zapros, config = twitter_token)
res_json <- fromJSON(toJSON(content(res)))
res_json$errors
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
res <- GET(zapros, config = twitter_token)
res <- GET(zapros, config = config(tw_token))
res_json <- fromJSON(toJSON(content(res)))
res_json
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
twitter_token <- config(token=tw_token)
res <- GET(zapros, config = config(tw_token))
res_json <- fromJSON(toJSON(content(res)))
res_json
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
res <- GET(zapros, config = config(tw_token))
res_json <- fromJSON(toJSON(content(res)))
res_json
config(tw_token)
apiKey <- "YMK6hg44dzNQlOTZDKG4apX39"
apiSecret <- "UnNuqNBJnCBI5wx1mo8KVfP4SYERahnXYD5OaX8XY5R4ZHziml"
twapp <- oauth_app("twappinfo", apiKey, apiSecret)
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
res <- GET(zapros, config = config(tw_token))
res_json <- fromJSON(toJSON(content(res)))
res_json
tw_token
GET(zapros, config = config(tw_token))
fromJSON(toJSON(content(GET(zapros, config = config(tw_token)))))
setup_twitter_oauth(apiKey,apiSecret)
tw = searchTwitter(word_enc,n=50,lang="ru")
twListToDF(tw)
Encoding(word) <- "UTF-8"
Encoding(word) <- "cp1251"
Encoding(word) <- "utf-8"
Encoding(word) <- "utf-8"
tw = searchTwitter(word_enc,n=50,lang="ru")
twListToDF(tw)
tw = searchTwitter(word,n=50,lang="ru")
twListToDF(tw)
tw = searchTwitter(word,n=10)
tw = searchTwitter(word,n=10,lang = "ru")
r <- twListToDF(tw)
r$text
word
curlUnescape(word)
tw = searchTwitter(curlEscape(word),n=10,lang = "ru")
r <- twListToDF(tw)
r$text
tw = searchTwitter(curlEscape(word),n=10)
Encoding(word)
word <- iconv(word, "cp1251", "utf-8")
word <- iconv(word, "cp1251", "UTF-8")
word <- "#запарк"
word <- iconv(word, "cp1251", "UTF-8")
tw = searchTwitter(word,n=10)
r <- twListToDF(tw)
r$text
tw_token <- oauth1.0_token(oauth_endpoints("twitter"), twapp)
word_enc <- curlEscape(word)
zapros <- paste0(base,word_enc)
res <- GET(zapros, config = config(tw_token))
res_json <- fromJSON(toJSON(content(res)))
res_json
word <- "#запаркнаходынке"
word <- iconv(word, "cp1251", "UTF-8")
tw = searchTwitter(word,n=10)
word <- "#паркукронштадтскогобульвара"
word <- iconv(word, "cp1251", "UTF-8")
tw = searchTwitter(word,n=10)
r <- twListToDF(tw)
r
r$created
r$screenName
tw = searchTwitter(word,n=10,lang = "ru")
word <- iconv(word, "cp1251", "UTF-8")
tw = searchTwitter(word,n=10,lang = "ru")
tw = searchTwitter(word,n=50,lang = "ru")
word <- "#паркдубки"
word <- iconv(word, "cp1251", "UTF-8")
tw = searchTwitter(word,n=50,lang = "ru")
r <- twListToDF(tw)
r$text
library(shiny)
install.packages("shiny")
library(manipulate)
install.packages("manipulate")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
myPlot()
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
abline(0, 2)
abline(0, 3)
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
data("airquality")
require(rCharts)
install.packages("rCharts")
dTable(airquality, sPaginationType = "full_numbers")
require(devtools)
install_github('rCharts', 'ramnathv')
load("D:/Sync/R/networks/VK/test/test.RData")
text_base <- wall_protest$all
text_base[1]
install.packages("rscopus")
require(rscopus)
text_base <- unlist(strsplit(text_base, split = ", "))
text_base2 <- grep(text_base, replace_non_ascii(text_base))
tb2 <- replace_non_ascii(text_base)
text_base <- wall_protest$all
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
library(tm)
library(topicmodels)
library(RTextTools)
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(toupper))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
usableText=str_replace_all(text_base,"[^[:graph:]]", " ")
require(stringr)
usableText=str_replace_all(text_base,"[^[:graph:]]", " ")
myCorpus_txt <- VCorpus(VectorSource(usableText), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
usableText <- iconv(usableText, "ASCII", "UTF-8", sub="")
?clean
text_base <- wall_protest$all
text_base <- sapply(text_base, function(x) tryTolower(x))
tryTolower = function(x)
{
# create missing value
# this is where the returned value will be
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
text_base <- wall_protest$all
text_base <- sapply(text_base, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
rm(myCorpus_txt)
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
stemCompletion_mod <- function(x,dict=corpuscopy) {
PlainTextDocument(stripWhitespace(paste(stemCompletion(unlist(strsplit(as.character(x)," ")),dictionary=dict, type="prevalent"),sep="", collapse=" ")))
}
?strsplit
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myCorpus_txt = tm_map(myCorpus_txt, stemCompletion_mod, dict = dictCorpus)
myCorpus_txt = tm_map(myCorpus_txt, stemCompletion_mod, dict = dictCorpus_txt)
myCorpus_txt <- lapply(myCorpus_txt, stemCompletion_mod(x, dict = dictCorpus_txt))
myCorpus_txt <- lapply(myCorpus_txt, function(x) stemCompletion_mod(x, dict = dictCorpus_txt))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 5, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 5, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 5, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
str(myDtm_txt)
rowTotals <- apply(myDtm_txt , 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
rowTotals <- apply(myDtm_txt , 1, sum)
rowTotals <- apply(myDtm_txt, 1, sum)
rowTotals <- apply(myDtm_txt, 2, sum)
empty.rows <- colSums(myDtm_txt)
em.ro <- rowsum(myDtm_txt)
rowTotals <- apply(myDtm_txt, 1, sum)
help("memory.size")
text_base <- wall_protest[1:1000,]$all
text_base <- sapply(text_base, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 5, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
rowTotals <- apply(dtm , 1, sum)
rowTotals <- apply(myDtm_txt, 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 5, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
r
Terms_txt = terms(my_TM, 5)
Terms_txt = terms(my_TM_txt, 5)
Terms_txt
Terms_txt = terms(my_TM_txt, 10)
Terms_txt
Terms_txt = as.matrix(terms(my_TM_txt, 10))
Terms_txt
topic_txt = as.matrix(topics(my_TM))
topic_txt = as.matrix(topics(my_TM_txt))
topic_txt
terms_txt = as.matrix(terms(my_TM_txt, 10))
topicProbabilities <- as.data.frame(my_TM_txt@gamma)
head(topicProbabilities)
?which.max
str(my_TM_txt)
?LDA
?sample
text_base <- wall_protest[sample(wall_protest, 1000),]$all
sample(wall_protest$own_id, 10)
text_base <- wall_protest[which(wall_protest$own_id == sample(wall_protest$own_id, 1000)),]$all
sample(wall_protest$own_id, 1000)
wall_protest[which(wall_protest$own_id == sample(wall_protest$own_id, 10)),]$all
sam <- sample(wall_protest$own_id, 1000)
text_base <- wall_protest[which(wall_protest$own_id == sam),]$all
text_base <- wall_protest[which(wall_protest$own_id == sam),]
View(text_base)
which(wall_protest$own_id == sam)
which(wall_protest$own_id = sam)
wall_protest[sam,"all"]
subset(wall_protest, own_id == sam)
sam
wall_protest <- cbind(wall_protest, row_id = 1:nrow(wall_protest))
sam <- sample(wall_protest$row_id, 1000)
text_base <- wall_protest[sam,"all"]
text_base <- sapply(text_base, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myCorpus_txt <- lapply(myCorpus_txt, function(x) stemCompletion_mod(x, dict = dictCorpus_txt))
rm(active, clear_fol, dupl, followers_data, groups_data, media, NoN_protest, num.news, repost_data, repost_data_shrink, soso_active, users_data, users_data_NoN, wall_parse_dat, examples, fields, group_ids, groups.ids, groups.ind, offset_const, offset_in, spisok, token, top_protest_chan, top_protest_chan_val, users, users_na, vers, vk, vk_auth, wa_se, zapros)
save.image("D:/Sync/R/networks/VK/text/text_work.RData")
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 4, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
topic_txt = as.matrix(topics(my_TM_txt))
terms_txt = as.matrix(terms(my_TM_txt, 10))
topicProbabilities <- as.data.frame(my_TM_txt@gamma)
terms_txt
save.image("D:/Sync/R/networks/VK/text/text_work.RData")
load("D:/Sync/R/networks/VK/text/text_work.RData")
library(tm)
library(topicmodels)
library(RTextTools)
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
class(myCorpus_txt)
class(myDtm_txt)
sam <- sample(wall_protest$row_id, 1000)
text_base <- wall_protest[sam,"all"]
text_base <- sapply(text_base, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myCorpus_txt <- tm_map(myCorpus_txt, stemCompletion_mod, dict = dictCorpus_txt)
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 4, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
rowTotals <- apply(myDtm_txt, 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 4, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
rowTotals <- apply(myDtm_txt, 1, sum)
sam <- sample(wall_protest$row_id, 1000)
text_base <- wall_protest[sam,"all"]
text_base <- sapply(text_base, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
rowTotals <- apply(myDtm_txt, 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 4, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
terms_txt = as.matrix(terms(my_TM_txt, 10))
topic_txt = as.matrix(topics(my_TM_txt))
terms_txt
save.image("D:/Sync/R/networks/VK/text/text_work.RData")
sam2 <- sample(wall_protest$row_id, 1000)
text_base2 <- wall_protest[sam,"all"]
text_base2 <- sapply(text_base, function(x) tryTolower(x))
sam2 <- sample(wall_protest$row_id, 10)
text_base2 <- wall_protest[sam2,"all"]
text_base2 <- sapply(text_base2, function(x) tryTolower(x))
myCorpus_txt <- VCorpus(VectorSource(text_base2), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myCorpus_txt <- tm_map(myCorpus_txt, stemCompletion_mod, dict = dictCorpus_txt)
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
rowTotals <- apply(myDtm_txt, 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
myCorpus_txt <- VCorpus(VectorSource(text_base2), readerControl = list(language = "ru"))
myCorpus_txt = tm_map(myCorpus_txt, content_transformer(tolower))
myCorpus_txt = tm_map(myCorpus_txt, removePunctuation)
myCorpus_txt = tm_map(myCorpus_txt, removeNumbers)
myCorpus_txt = tm_map(myCorpus_txt, removeWords, stopwords('russian'))
myCorpus_txt = tm_map(myCorpus_txt, stripWhitespace)
dictCorpus_txt = myCorpus_txt
myCorpus_txt = tm_map(myCorpus_txt, stemDocument, language = "ru")
myCorpus_txt <- tm_map(myCorpus_txt, stemCompletion_mod, dict = dictCorpus_txt)
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
my_TM_txt = LDA(myDtm_txt, method = "Gibbs", k = 4, control = list(nstart = 5, seed = list(2003,5,63,100001,765), burnin = 4000, iter = 2000, thin = 500))
rowTotals <- apply(myDtm_txt, 1, sum)
empty.rows <- myDtm_txt[rowTotals == 0, ]$dimnames[1][[1]]
myCorpus_txt <- tm_filter(
myCorpus_txt,
FUN = function(doc) !is.element(meta(doc)$id, empty.rows))
myDtm_txt = DocumentTermMatrix(myCorpus_txt, control = list(minWordLength = 3))
load("D:/Sync/R/networks/VK/text/text_work.RData")
View(Terms_txt)
require(rCharts)
require(Hmisc)
setwd("D:/Sync/Левада-Центр/Российско-грузинский опрос/charts")
rus <- read.csv("russian.csv")
View(rus)
rus <- read.csv("russian.csv", sep = ";")
View(rus)
geo <- read.csv("georgian.csv", sep = ";")
names(geo)
colnames(geo) <- c("know", "age")
names(geo)
rus$nation <- as.factor("rus")
View(rus)
geo$nation <- as.factor("geo")
?merge
library(dplyr)
library(plye)
library(plyr)
library(dplyr)
geo <- select(geo, age, know)
names(geo)
names(rus)
geo$nation <- as.factor("geo")
uni <- rbind(rus, geo)
?cut2
cut_points <- c(25,40,55)
uni$age_gr <- cut2(uni$age, cuts = cut_points)
head(uni)
summary(uni$age_gr)
nrow(uni[which(uni$age < 18),])
table(uni[which(uni$age < 18),"nation"])
uni <- uni[which(uni$age >= 18),]
cut_points <- c(25,40,55)
uni$age_gr <- cut2(uni$age, cuts = cut_points)
summary(uni$age_gr)
?revalue
uni$age_gr <- revalue(uni$age_gr, c("[ 18, 25)" = "18-24", "[ 25, 40)" = "25-39", "[ 40, 55)" = "40-54", "[ 55,102]" = ">55"))
summary(uni$age_gr)
str(uni)
?factor
summary(uni$know)
uni$know2 <- factor(uni$know, levels = c("1","2","3","4","9"), labels = c("много знаю", "знаю немного", "знаю мало", "ничего не знаю", "затруд.ответить"))
summary(uni$know2)
summary(as.factor(uni$know))
uni$know2 <- factor(uni$know, labels = c("много знаю", "знаю немного", "знаю мало", "ничего не знаю", "затруд.ответить"))
summary(uni$know2)
uni$know <- factor(uni$know, labels = c("много знаю", "знаю немного", "знаю мало", "ничего не знаю", "затруд.ответить"))
uni <- uni[,-"know2"]
View(uni)
uni <- uni[,-5]
?nPlot
??nPlot
prop.table(uni, 1)
prop.table(uni, 2)
View(uni)
prop.table(uni[,2], 2)
prop.table(uni[,2], 1)
prop.table(uni[,,2], 1)
prop.table(uni[,,1], 1)
prop.table(uni[,,1])
prop.table(uni[,,2])
prop.table(uni[,1])
prop.table(uni[,2])
HairEyeColor
hec <- HairEyeColor
prop.table(HairEyeColor[,,1],2)
hec
prop.table(HairEyeColor[,,1],2)
prop.table(HairEyeColor[,,1],1)
ch <- nPlot(~know, data = uni, type = multiBarChart)
ch <- nPlot(~know, data = uni, type = "multiBarChart")
ch
Encoding(uni$know) <- "UTF-8"
ch <- nPlot(Percent~know, data = uni, type = "multiBarChart")
prop.table(uni$age)
prop.table(uni$age, 2)
prop.table(uni$age, 1)
prop.table(uni$know)
install.packages("ggvis")
?xtabs
xtabs(~know, data = uni)
prop.table(xtabs(~know, data = uni))
prop.table(xtabs(age_gr~know, data = uni))
prop.table(xtabs(age~know, data = uni))
xtabs(age~know, data = uni)
xtabs(~know + age_gr, data = uni)
save.image("D:/Sync/Левада-Центр/Российско-грузинский опрос/charts/chart_mass.RData")
